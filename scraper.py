import requests, re, socket, time, concurrent.futures
from datetime import datetime

# Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ø¯Ø«Ø© Ø¨Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù€ RAW Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
SOURCES = [
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ (ØªÙ… ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù€ RAW Ù„ÙŠÙ‚Ø±Ø£ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¨Ø§Ø´Ø±Ø©)
    "https://raw.githubusercontent.com/Badr-cx/icone-wegoo/main/VERIFIED_CANNON.cfg",
    # Ø±ÙˆØ§Ø¨Ø· ØªÙŠÙ„ÙŠØºØ±Ø§Ù… Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹
    "https://t.me/s/Free_Cccam_Server_Daily",
    "https://t.me/s/vipsat_net",
    "https://t.me/s/smart_cccam",
    # Ù…ÙˆØ§Ù‚Ø¹ Ø£ÙˆØ±ÙˆØ¨ÙŠØ© (ØªØ¹Ø·ÙŠ Ø³ÙŠØ±ÙØ±Ø§Øª Ø·Ø§Ø²Ø¬Ø©)
    "https://cccamia.com/cccamfree1/",
    "https://cccamcard.com/free-cccam-server.php",
    "https://raw.githubusercontent.com/yebekhe/TV-Logo/main/cccam.txt"
]

def verify_server(line):
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·Ø± Ù…Ù† Ø£ÙŠ Ø´ÙˆØ§Ø¦Ø¨ HTML Ø£Ùˆ Ù…Ø³Ø§ÙØ§Øª Ø²Ø§Ø¦Ø¯Ø©
    line = re.sub('<[^<]+?>', '', line).strip()
    match = re.search(r'C:\s*([a-zA-Z0-9\-\.]+)\s+(\d+)\s+(\S+)\s+(\S+)', line, re.I)
    if not match: return None
    
    host, port, user, passwd = match.groups()
    if any(f in host.lower() for f in ['127.0.0.1', 'nassim', 'stream']): return None

    try:
        start = time.perf_counter()
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§ØªØµØ§Ù„ Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ø§Ù„Ù…Ù†ÙØ° (Port) Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø´ØªØºØ§Ù„ Ø§Ù„Ø³ÙŠØ±ÙØ±
        with socket.create_connection((host, int(port)), timeout=2.0) as sock:
            latency = int((time.perf_counter() - start) * 1000)
            return (latency, host, port, user, passwd)
    except:
        return None

def run_scraper():
    print(f"ğŸš€ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù† {len(SOURCES)} Ù…ØµØ§Ø¯Ø±...")
    all_raw = []
    
    session = requests.Session()
    # Ø¥Ø¶Ø§ÙØ© User-Agent Ù„ØªØ¨Ø¯Ùˆ ÙƒÙ…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ ÙˆØªØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

    for url in SOURCES:
        try:
            r = session.get(url, timeout=15)
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ø·Ø± C: Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
            matches = re.findall(r'C:\s*[a-zA-Z0-9\-\.]+\s+\d+\s+\S+\s+\S+', r.text, re.I)
            
            # ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª ÙÙŠ ØªÙŠÙ„ÙŠØºØ±Ø§Ù… Ø¨Ø¯ÙˆÙ† Ø­Ø±Ù C:
            if not matches:
                extra = re.findall(r'([a-zA-Z0-9\-\.]+\s+\d+\s+[a-zA-Z0-9\-\.]+\s+[a-zA-Z0-9\-\.]+)', r.text)
                for e in extra:
                    if e.split()[1].isdigit(): matches.append(f"C: {e}")
            
            all_raw.extend(matches)
            print(f"âœ… {url.split('/')[-1]}: ÙˆØ¬Ø¯Ù†Ø§ {len(matches)} Ø³ÙŠØ±ÙØ±")
        except: continue

    unique_list = list(set(all_raw))
    print(f"ğŸ§ª Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø¬ÙˆØ¯Ø© {len(unique_list)} Ø³ÙŠØ±ÙØ±... Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹.")

    # ÙØ­Øµ 150 Ø³ÙŠØ±ÙØ± ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ (Ø³Ø±Ø¹Ø© ØµØ§Ø±ÙˆØ®ÙŠØ©)
    with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:
        results = [r for r in executor.map(verify_server, unique_list) if r]

    # ØªØ±ØªÙŠØ¨ Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ Ù„Ù„Ø£Ø¨Ø·Ø£
    results.sort(key=lambda x: x[0])

    if results:
        with open("ncam.server", "w", encoding="utf-8") as f:
            f.write(f"### GENERATED | {datetime.now().strftime('%H:%M')} ###\n")
            for i, (lat, host, port, user, passwd) in enumerate(results[:50]):
                f.write(f"\n[reader]\nlabel = Server_{i+1}_{lat}ms\nprotocol = cccam\ndevice = {host},{port}\nuser = {user}\npassword = {passwd}\ngroup = 1\ncccversion = 2.3.2\nccckeepalive = 1\n")
        
        with open("CCcam.cfg", "w", encoding="utf-8") as f:
            for lat, host, port, user, passwd in results[:50]:
                f.write(f"C: {host} {port} {user} {passwd} # {lat}ms\n")
        
        print(f"âœ¨ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ø³ÙŠØ±ÙØ± Ø´ØºØ§Ù„ 100%!")
        print(f"ğŸ“‚ Ø§Ù„Ù…Ù„ÙØ§Øª ncam.server Ùˆ CCcam.cfg Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†.")
    else:
        print("âŒ Ù„Ù… Ù†Ø¬Ø¯ Ø£ÙŠ Ø³ÙŠØ±ÙØ± Ø´ØºØ§Ù„ Ø­Ø§Ù„ÙŠØ§Ù‹. Ø±Ø¨Ù…Ø§ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØªÙˆÙ‚ÙØª.")

if __name__ == "__main__":
    run_scraper()
